---
title: "Data Analysis"
output:
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

Import necessary libraries.

```{r}

# install_keras() - don't want to install keras until we've determined
# we want to use it because keras is quite large.

library(readr)
library(data.table)
library(TermDataAnalysis)
library(keras3)
library(parsnip)
library(tidyverse)
library(modeldata)
library(randomForest)
library(e1071)
library(tensorflow)

# devtools::install_github("rstudio/tensorflow")

```

Then, we can import the data.

```{r}
#colTypes <- c("int", "num", "int", "num", "int", "num", 
#  "num", "num", "num", "num", "factor", "int",
#  "int", "int", "int", "factor", "factor", "factor")

onlineShopperData <- fread("data/online_shoppers_intention.csv")

data_processed <- onlineShopperData[, Revenue := as.factor(Revenue)]



```

Pre-process the data

```{r}

data_processed <- data_processed[, total_Duration := Administrative_Duration + Informational_Duration + ProductRelated_Duration]

data_processed <- data_processed[, proportion_of_duration := ProductRelated_Duration / total_Duration]

data_processed <- data_processed[, c("OperatingSystems", 
                    "Browser", "Administrative", "Informational", 
                    "ProductRelated", "Administrative_Duration", 
                    "Informational_Duration", "ProductRelated_Duration") := NULL]

```

Build models identical to Sakar, but on our preprocessed version of the
data We need to try building 1. MultiLayer Perceptron 2. SVM 3. Decision
Trees

Initially, we'll split the data into 70% training set and 30%
validation, just like in Sakar.

```{r}

# source for examples https://www.r-bloggers.com/2024/09/how-to-split-a-data-frame-in-r-a-comprehensive-guide-for-beginners/
set.seed(123)

sample_size <- floor(0.7 * nrow(data_processed))
train_indices <- sample(seq_len(nrow(data_processed)), size = sample_size)

train_data <- data_processed[train_indices,]
test_data <- data_processed[-train_indices,]


```

1.  Multilayer Perceptron TODO - In Progress

Define the first mlp TODO - In Progress

```{r}

# Make a simple mlp

mlp1 = keras_model_sequential()
mlp1 %>%
  layer_dense(units=10, activation='relu', input_shape = c(11))

mlp1 %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_sgd(),
  metrics = c('accuracy')
)

```

Shape the data for use in the MLP.

```{r}

train_x_dt <- train_data[,c("BounceRates", "ExitRates", "PageValues", "SpecialDay", "Month", "Region", "TrafficType", "VisitorType", "Weekend", "total_Duration", "proportion_of_duration")]

train_y_dt <- train_data[,"Revenue"]

train_x <- array(data = c(unlist(train_x_dt)), dim = c(nrow(train_x_dt), 11), dimnames = list(rownames(train_x_dt), colnames(train_x_dt)))

train_x <- as.matrix(train_x_dt)
train_y <- as.matrix(train_data[,"Revenue"])

# train_x <- array_reshape(train_x, c(nrow(train_x), 11))
# x_test <- array_reshape(train_y, c(nrow(x_test), 784))

```

Train the first mlp TODO - In Progress

```{r}

mlp1_fit <- mlp1 %>% fit(train_x, train_y, epochs = 30, batch_size = 128, 
  validation_split = 0.2)

```

2.  SVM

```{r}

## classification mode
# default with factor response:
svm_model <- svm(Revenue ~ ., data = train_data)

print(svm_model)
summary(svm_model)

# test with train data
pred <- predict(svm_model, train_data)


```

3.  Random Forest

```{r}

## Classification:
set.seed(71)
train_data.rf <- randomForest(Revenue ~ ., data=train_data, importance=TRUE,
                        proximity=TRUE, na.action = na.omit)
print(train_data.rf)

```

```{r}
check <- predict(train_data.rf, test_data) # compare to test_data$Revenue
```


Compare results

Use oversampling

Reconstruct models

Compare

If time use SMOTE?

Reconstruct models

Compare

Significantly reduce the size of the training set

Rebuild the models

Compare
